# Telegraf Configuration
#
# Telegraf is entirely plugin driven. All metrics are gathered from the
# declared inputs, and sent to the declared outputs.
#
# Plugins must be declared in here to be active.
# To deactivate a plugin, comment out the name and any variables.
#
# Use 'telegraf -config telegraf.conf -test' to see what metrics a config
# file would generate.
#
# Environment variables can be used anywhere in this config file, simply surround
# them with ${}. For strings the variable must be within quotes (ie, "${STR_VAR}"),
# for numbers and booleans they should be plain (ie, ${INT_VAR}, ${BOOL_VAR})


# Global tags can be specified here in key="value" format.
[global_tags]
  # dc = "us-east-1" # will tag all metrics with dc=us-east-1
  # rack = "1a"
  ## Environment variables can be used as tags, and throughout the config file
  # user = "$USER"

# Configuration for telegraf agent
[agent]
  ## Default data collection interval for all inputs
  interval = "{{ telegraf_collection_interval }}"
  ## Rounds collection interval to 'interval'
  ## ie, if interval="10s" then always collect on :00, :10, :20, etc.
  round_interval = true

  ## Telegraf will send metrics to outputs in batches of at most
  ## metric_batch_size metrics.
  ## This controls the size of writes that Telegraf sends to output plugins.
  metric_batch_size = {{ telegraf_metric_batch_size }}

  ## Maximum number of unwritten metrics per output.  Increasing this value
  ## allows for longer periods of output downtime without dropping metrics at the
  ## cost of higher maximum memory usage.
  metric_buffer_limit = {{ telegraf_metric_buffer_limit }}

  ## Collection jitter is used to jitter the collection by a random amount.
  ## Each plugin will sleep for a random time within jitter before collecting.
  ## This can be used to avoid many plugins querying things like sysfs at the
  ## same time, which can have a measurable effect on the system.
  collection_jitter = "0s"

  ## Collection offset is used to shift the collection by the given amount.
  ## This can be be used to avoid many plugins querying constraint devices
  ## at the same time by manually scheduling them in time.
  # collection_offset = "0s"

  ## Default flushing interval for all outputs. Maximum flush_interval will be
  ## flush_interval + flush_jitter
  flush_interval = "{{ telegraf_flush_interval }}"
  ## Jitter the flush interval by a random amount. This is primarily to avoid
  ## large write spikes for users running a large number of telegraf instances.
  ## ie, a jitter of 5s and interval 10s means flushes will happen every 10-15s
  flush_jitter = "{{ telegraf_flush_jitter }}"

  ## Collected metrics are rounded to the precision specified. Precision is
  ## specified as an interval with an integer + unit (e.g. 0s, 10ms, 2us, 4s).
  ## Valid time units are "ns", "us" (or "Âµs"), "ms", "s".
  ##
  ## By default or when set to "0s", precision will be set to the same
  ## timestamp order as the collection interval, with the maximum being 1s:
  ##   ie, when interval = "10s", precision will be "1s"
  ##       when interval = "250ms", precision will be "1ms"
  ##
  ## Precision will NOT be used for service inputs. It is up to each individual
  ## service input to set the timestamp at the appropriate precision.
  precision = "0s"

  ## Log at debug level.
  # debug = false
  ## Log only error level messages.
  # quiet = false
{% if telegraf_debug_enable == True %}
  debug = true
  quiet = false
{% endif %}

  ## Log target controls the destination for logs and can be one of "file",
  ## "stderr" or, on Windows, "eventlog".  When set to "file", the output file
  ## is determined by the "logfile" setting.
  # logtarget = "file"

  ## Name of the file to be logged to when using the "file" logtarget.  If set to
  ## the empty string then logs are written to stderr.
  # logfile = ""

  ## The logfile will be rotated after the time interval specified.  When set
  ## to 0 no time based rotation is performed.  Logs are rotated only when
  ## written to, if there is no log activity rotation may be delayed.
  # logfile_rotation_interval = "0h"

  ## The logfile will be rotated when it becomes larger than the specified
  ## size.  When set to 0 no size based rotation is performed.
  # logfile_rotation_max_size = "0MB"

  ## Maximum number of rotated archives to keep, any older logs are deleted.
  ## If set to -1, no archives are removed.
  # logfile_rotation_max_archives = 5

  ## Pick a timezone to use when logging or type 'local' for local time.
  ## Example: America/Chicago
  # log_with_timezone = ""

  ## Override default hostname, if empty use os.Hostname()
  hostname = "{{ inventory_hostname }}"
  ## If set to true, do no set the "host" tag in the telegraf agent.
  omit_hostname = false

  ## Method of translating SNMP objects. Can be "netsnmp" (deprecated) which
  ## translates by calling external programs snmptranslate and snmptable,
  ## or "gosmi" which translates using the built-in gosmi library.
  # snmp_translator = "netsnmp"

  ## Name of the file to load the state of plugins from and store the state to.
  ## If uncommented and not empty, this file will be used to save the state of
  ## stateful plugins on termination of Telegraf. If the file exists on start,
  ## the state in the file will be restored for the plugins.
  # statefile = ""

###############################################################################
#                            OUTPUT PLUGINS                                   #
###############################################################################

{% if telegraf_influxdb2_url != '' %}
# # Configuration for sending metrics to InfluxDB 2.0
[[outputs.influxdb_v2]]
#   ## The URLs of the InfluxDB cluster nodes.
#   ##
#   ## Multiple URLs can be specified for a single cluster, only ONE of the
#   ## urls will be written to each interval.
#   ##   ex: urls = ["https://us-west-2-1.aws.cloud2.influxdata.com"]
#   urls = ["http://127.0.0.1:8086"]
    urls = ["{{ telegraf_influxdb2_url }}"]
#
#   ## Token for authentication.
#   token = ""
    token = "{{ telegraf_influxdb2_token }}"
#
#   ## Organization is the name of the organization you wish to write to.
#   organization = ""
#
#   ## Destination bucket to write into.
#   bucket = ""
#
#   ## The value of this tag will be used to determine the bucket.  If this
#   ## tag is not set the 'bucket' option is used as the default.
#   # bucket_tag = ""
#
#   ## If true, the bucket tag will not be added to the metric.
#   # exclude_bucket_tag = false
#
#   ## Timeout for HTTP messages.
#   # timeout = "5s"
#
#   ## Additional HTTP headers
#   # http_headers = {"X-Special-Header" = "Special-Value"}
#
#   ## HTTP Proxy override, if unset values the standard proxy environment
#   ## variables are consulted to determine which proxy, if any, should be used.
#   # http_proxy = "http://corporate.proxy:3128"
#
#   ## HTTP User-Agent
#   # user_agent = "telegraf"
#
#   ## Content-Encoding for write request body, can be set to "gzip" to
#   ## compress body or "identity" to apply no encoding.
#   # content_encoding = "gzip"
#
#   ## Enable or disable uint support for writing uints influxdb 2.0.
#   # influx_uint_support = false
#
#   ## HTTP/2 Timeouts
#   ## The following values control the HTTP/2 client's timeouts. These settings
#   ## are generally not required unless a user is seeing issues with client
#   ## disconnects. If a user does see issues, then it is suggested to set these
#   ## values to "15s" for ping timeout and "30s" for read idle timeout and
#   ## retry.
#   ##
#   ## Note that the timer for read_idle_timeout begins at the end of the last
#   ## successful write and not at the beginning of the next write.
#   # ping_timeout = "0s"
#   # read_idle_timeout = "0s"
#
#   ## Optional TLS Config for use on HTTP connections.
#   # tls_ca = "/etc/telegraf/ca.pem"
#   # tls_cert = "/etc/telegraf/cert.pem"
#   # tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
{% endif %}


{% if telegraf_influxdb_url != '' %}
# # Configuration for sending metrics to InfluxDB
[[outputs.influxdb]]
#   ## The full HTTP or UDP URL for your InfluxDB instance.
#   ##
#   ## Multiple URLs can be specified for a single cluster, only ONE of the
#   ## urls will be written to each interval.
#   # urls = ["unix:///var/run/influxdb.sock"]
#   # urls = ["udp://127.0.0.1:8089"]
#   # urls = ["http://127.0.0.1:8086"]
      urls = ["{{ telegraf_influxdb_url }}"]
#
#   ## The target database for metrics; will be created as needed.
#   ## For UDP url endpoint database needs to be configured on server side.
#   # database = "telegraf"
      database = "{{ telegraf_influxdb_db_name }}"
#
#   ## The value of this tag will be used to determine the database.  If this
#   ## tag is not set the 'database' option is used as the default.
#   # database_tag = ""
#
#   ## If true, the 'database_tag' will not be included in the written metric.
#   # exclude_database_tag = false
#
#   ## If true, no CREATE DATABASE queries will be sent.  Set to true when using
#   ## Telegraf with a user without permissions to create databases or when the
#   ## database already exists.
#   # skip_database_creation = false
#
#   ## Name of existing retention policy to write to.  Empty string writes to
#   ## the default retention policy.  Only takes effect when using HTTP.
#   # retention_policy = ""
#
#   ## The value of this tag will be used to determine the retention policy.  If this
#   ## tag is not set the 'retention_policy' option is used as the default.
#   # retention_policy_tag = ""
#
#   ## If true, the 'retention_policy_tag' will not be included in the written metric.
#   # exclude_retention_policy_tag = false
#
#   ## Write consistency (clusters only), can be: "any", "one", "quorum", "all".
#   ## Only takes effect when using HTTP.
#   # write_consistency = "any"
#
#   ## Timeout for HTTP messages.
#   # timeout = "5s"
#
#   ## HTTP Basic Auth
#   # username = "telegraf"
#   # password = "metricsmetricsmetricsmetrics"
      username = "{{ telegraf_influxdb_name }}"
      password = "{{ telegraf_influxdb_pass }}"
#
#   ## HTTP User-Agent
#   # user_agent = "telegraf"
#
#   ## UDP payload size is the maximum packet size to send.
#   # udp_payload = "512B"
#
#   ## Optional TLS Config for use on HTTP connections.
#   # tls_ca = "/etc/telegraf/ca.pem"
#   # tls_cert = "/etc/telegraf/cert.pem"
#   # tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
#
#   ## HTTP Proxy override, if unset values the standard proxy environment
#   ## variables are consulted to determine which proxy, if any, should be used.
#   # http_proxy = "http://corporate.proxy:3128"
#
#   ## Additional HTTP headers
#   # http_headers = {"X-Special-Header" = "Special-Value"}
#
#   ## HTTP Content-Encoding for write request body, can be set to "gzip" to
#   ## compress body or "identity" to apply no encoding.
#   # content_encoding = "gzip"
#
#   ## When true, Telegraf will output unsigned integers as unsigned values,
#   ## i.e.: "42u".  You will need a version of InfluxDB supporting unsigned
#   ## integer values.  Enabling this option will result in field type errors if
#   ## existing data has been written.
#   # influx_uint_support = false
{% endif %}


{% if telegraf_prometheusremotewrite_url != '' %}
# # A plugin that can transmit metrics over HTTP
[[outputs.http]]
#   ## URL is the address to send metrics to
#   url = "http://127.0.0.1:8080/telegraf"
    url = "{{ telegraf_prometheusremotewrite_url }}"
#
#   ## Timeout for HTTP message
#   # timeout = "5s"
      timeout = "{{ telegraf_prometheusremotewrite_timeout }}"
#
#   ## HTTP method, one of: "POST" or "PUT" or "PATCH"
#   # method = "POST"
#
#   ## HTTP Basic Auth credentials
#   # username = "username"
#   # password = "pa$$word"
#
#   ## OAuth2 Client Credentials Grant
#   # client_id = "clientid"
#   # client_secret = "secret"
#   # token_url = "https://indentityprovider/oauth2/v1/token"
#   # audience = ""
#   # scopes = ["urn:opc:idm:__myscopes__"]
#
#   ## Goole API Auth
#   # google_application_credentials = "/etc/telegraf/example_secret.json"
#
#   ## HTTP Proxy support
#   # use_system_proxy = false
#   # http_proxy_url = ""
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem"
#   # tls_cert = "/etc/telegraf/cert.pem"
#   # tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
#
#   ## Optional Cookie authentication
#   # cookie_auth_url = "https://localhost/authMe"
#   # cookie_auth_method = "POST"
#   # cookie_auth_username = "username"
#   # cookie_auth_password = "pa$$word"
#   # cookie_auth_headers = '{"Content-Type": "application/json", "X-MY-HEADER":"hello"}'
#   # cookie_auth_body = '{"username": "user", "password": "pa$$word", "authenticate": "me"}'
#   ## cookie_auth_renewal not set or set to "0" will auth once and never renew the cookie
#   # cookie_auth_renewal = "5m"
#
#   ## Data format to output.
#   ## Each data format has it's own unique set of configuration options, read
#   ## more about them here:
#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md
#   # data_format = "influx"
      data_format = "prometheusremotewrite"
#
#   ## Use batch serialization format (default) instead of line based format.
#   ## Batch format is more efficient and should be used unless line based
#   ## format is really needed.
#   # use_batch_format = true
#
#   ## HTTP Content-Encoding for write request body, can be set to "gzip" to
#   ## compress body or "identity" to apply no encoding.
#   # content_encoding = "identity"
#
#   ## MaxIdleConns controls the maximum number of idle (keep-alive)
#   ## connections across all hosts. Zero means no limit.
#   # max_idle_conn = 0
#
#   ## MaxIdleConnsPerHost, if non-zero, controls the maximum idle
#   ## (keep-alive) connections to keep per-host. If zero,
#   ## DefaultMaxIdleConnsPerHost is used(2).
#   # max_idle_conn_per_host = 2
#
#   ## Idle (keep-alive) connection timeout.
#   ## Maximum amount of time before idle connection is closed.
#   ## Zero means no limit.
#   # idle_conn_timeout = 0
#
#   ## Amazon Region
#   #region = "us-east-1"
#
#   ## Amazon Credentials
#   ## Amazon Credentials are not built unless the following aws_service
#   ## setting is set to a non-empty string. It may need to match the name of
#   ## the service output to as well
#   #aws_service = "execute-api"
#
#   ## Credentials are loaded in the following order
#   ## 1) Web identity provider credentials via STS if role_arn and web_identity_token_file are specified
#   ## 2) Assumed credentials via STS if role_arn is specified
#   ## 3) explicit credentials from 'access_key' and 'secret_key'
#   ## 4) shared profile from 'profile'
#   ## 5) environment variables
#   ## 6) shared credentials file
#   ## 7) EC2 Instance Profile
#   #access_key = ""
#   #secret_key = ""
#   #token = ""
#   #role_arn = ""
#   #web_identity_token_file = ""
#   #role_session_name = ""
#   #profile = ""
#   #shared_credential_file = ""
#
#   ## Optional list of statuscodes (<200 or >300) upon which requests should not be retried
#   # non_retryable_statuscodes = [409, 413]
#
#   ## NOTE: Due to the way TOML is parsed, tables must be at the END of the
#   ## plugin definition, otherwise additional config options are read as part of
#   ## the table
#
#   ## Additional HTTP headers
#   # [outputs.http.headers]
#   #   ## Should be set manually to "application/json" for json data_format
#   #   Content-Type = "text/plain; charset=utf-8"
      [outputs.http.headers]
        Content-Type = "application/x-protobuf"
        Content-Encoding = "snappy"
        X-Prometheus-Remote-Write-Version = "0.1.0"
        Authorization = "Token {{ telegraf_prometheusremotewrite_token }}"
{% endif %}

###############################################################################
#                            INPUT PLUGINS                                    #
###############################################################################


# Read metrics about cpu usage
[[inputs.cpu]]
  ## Whether to report per-cpu stats or not
  percpu = true
  ## Whether to report total system cpu stats or not
  totalcpu = true
  ## If true, collect raw CPU time metrics
  collect_cpu_time = false
  ## If true, compute and report the sum of all non-idle CPU states
  ## NOTE: The resulting 'time_active' field INCLUDES 'iowait'!
  report_active = false
  ## If true and the info is available then add core_id and physical_id tags
  core_tags = false


# Read metrics about disk usage by mount point
[[inputs.disk]]
  ## By default stats will be gathered for all mount points.
  ## Set mount_points will restrict the stats to only the specified mount points.
  # mount_points = ["/"]

  ## Ignore mount points by filesystem type.
  ignore_fs = ["tmpfs", "devtmpfs", "devfs", "iso9660", "overlay", "aufs", "squashfs"]

  ## Ignore mount points by mount options.
  ## The 'mount' command reports options of all mounts in parathesis.
  ## Bind mounts can be ignored with the special 'bind' option.
  # ignore_mount_opts = []


# Read metrics about disk IO by device
[[inputs.diskio]]
  ## By default, telegraf will gather stats for all devices including
  ## disk partitions.
  ## Setting devices will restrict the stats to the specified devices.
  ## NOTE: Globbing expressions (e.g. asterix) are not supported for
  ##       disk synonyms like '/dev/disk/by-id'.
  # devices = ["sda", "sdb", "vd*", "/dev/disk/by-id/nvme-eui.00123deadc0de123"]
  ## Uncomment the following line if you need disk serial numbers.
  # skip_serial_number = false
  #
  ## On systems which support it, device metadata can be added in the form of
  ## tags.
  ## Currently only Linux is supported via udev properties. You can view
  ## available properties for a device by running:
  ## 'udevadm info -q property -n /dev/sda'
  ## Note: Most, but not all, udev properties can be accessed this way. Properties
  ## that are currently inaccessible include DEVTYPE, DEVNAME, and DEVPATH.
  # device_tags = ["ID_FS_TYPE", "ID_FS_USAGE"]
  #
  ## Using the same metadata source as device_tags, you can also customize the
  ## name of the device via templates.
  ## The 'name_templates' parameter is a list of templates to try and apply to
  ## the device. The template may contain variables in the form of '$PROPERTY' or
  ## '${PROPERTY}'. The first template which does not contain any variables not
  ## present for the device is used as the device name tag.
  ## The typical use case is for LVM volumes, to get the VG/LV name instead of
  ## the near-meaningless DM-0 name.
  # name_templates = ["$ID_FS_LABEL","$DM_VG_NAME/$DM_LV_NAME"]


# Plugin to collect various Linux kernel statistics.
# This plugin ONLY supports Linux
[[inputs.kernel]]
  ## Additional gather options
  ## Possible options include:
  ## * ksm - kernel same-page merging
  # collect = []


# Read metrics about memory usage
[[inputs.mem]]
  # no configuration


# Get the number of processes and group them by status
# This plugin ONLY supports non-Windows
[[inputs.processes]]
  ## Use sudo to run ps command on *BSD systems. Linux systems will read
  ## /proc, so this does not apply there.
  # use_sudo = false


# Read metrics about swap memory usage
# This plugin ONLY supports Linux
[[inputs.swap]]
  # no configuration


# Read metrics about system load & uptime
#[[inputs.system]]
  # no configuration

{% if telegraf_apcupsd_enable %}
# # Monitor APC UPSes connected to apcupsd
[[inputs.apcupsd]]
#   # A list of running apcupsd server to connect to.
#   # If not provided will default to tcp://127.0.0.1:3551
#   servers = ["tcp://127.0.0.1:3551"]
  servers = ["tcp://127.0.0.1:3551"]
#
#   ## Timeout for dialing server.
#   timeout = "5s"
{% endif %}


{% if telegraf_docker_enable %}
# # Read metrics about docker containers
[[inputs.docker]]
#   ## Docker Endpoint
#   ##   To use TCP, set endpoint = "tcp://[ip]:[port]"
#   ##   To use environment variables (ie, docker-machine), set endpoint = "ENV"
#   endpoint = "unix:///var/run/docker.sock"
  endpoint = "unix:///var/run/docker.sock"
#
#   ## Set to true to collect Swarm metrics(desired_replicas, running_replicas)
#   ## Note: configure this in one of the manager nodes in a Swarm cluster.
#   ## configuring in multiple Swarm managers results in duplication of metrics.
#   gather_services = false
#
#   ## Only collect metrics for these containers. Values will be appended to
#   ## container_name_include.
#   ## Deprecated (1.4.0), use container_name_include
#   container_names = []
#
#   ## Set the source tag for the metrics to the container ID hostname, eg first 12 chars
#   source_tag = false
#
#   ## Containers to include and exclude. Collect all if empty. Globs accepted.
#   container_name_include = []
#   container_name_exclude = []
#
#   ## Container states to include and exclude. Globs accepted.
#   ## When empty only containers in the "running" state will be captured.
#   ## example: container_state_include = ["created", "restarting", "running", "removing", "paused", "exited", "dead"]
#   ## example: container_state_exclude = ["created", "restarting", "running", "removing", "paused", "exited", "dead"]
#   # container_state_include = []
#   # container_state_exclude = []
#
#   ## Objects to include for disk usage query
#   ## Allowed values are "container", "image", "volume"
#   ## When empty disk usage is excluded
#   storage_objects = []
#
#   ## Timeout for docker list, info, and stats commands
#   timeout = "5s"
#
#   ## Whether to report for each container per-device blkio (8:0, 8:1...),
#   ## network (eth0, eth1, ...) and cpu (cpu0, cpu1, ...) stats or not.
#   ## Usage of this setting is discouraged since it will be deprecated in favor of 'perdevice_include'.
#   ## Default value is 'true' for backwards compatibility, please set it to 'false' so that 'perdevice_include' setting
#   ## is honored.
#   perdevice = true
  perdevice = false
#
#   ## Specifies for which classes a per-device metric should be issued
#   ## Possible values are 'cpu' (cpu0, cpu1, ...), 'blkio' (8:0, 8:1, ...) and 'network' (eth0, eth1, ...)
#   ## Please note that this setting has no effect if 'perdevice' is set to 'true'
#   # perdevice_include = ["cpu"]
  perdevice_include = []
#
#   ## Whether to report for each container total blkio and network stats or not.
#   ## Usage of this setting is discouraged since it will be deprecated in favor of 'total_include'.
#   ## Default value is 'false' for backwards compatibility, please set it to 'true' so that 'total_include' setting
#   ## is honored.
#   total = false
  total = true
#
#   ## Specifies for which classes a total metric should be issued. Total is an aggregated of the 'perdevice' values.
#   ## Possible values are 'cpu', 'blkio' and 'network'
#   ## Total 'cpu' is reported directly by Docker daemon, and 'network' and 'blkio' totals are aggregated by this plugin.
#   ## Please note that this setting has no effect if 'total' is set to 'false'
#   # total_include = ["cpu", "blkio", "network"]
  total_include = ["cpu", "blkio", "network"]
#
#   ## docker labels to include and exclude as tags.  Globs accepted.
#   ## Note that an empty array for both will include all labels as tags
#   docker_label_include = []
#   docker_label_exclude = []
#
#   ## Which environment variables should we use as a tag
#   tag_env = ["JAVA_HOME", "HEAP_SIZE"]
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem"
#   # tls_cert = "/etc/telegraf/cert.pem"
#   # tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
{% endif %}


{% if telegraf_node_urls | length > 0 %}
# # HTTP/HTTPS request given an address a method and a timeout
[[inputs.http_response]]
#   ## Deprecated in 1.12, use 'urls'
#   ## Server address (default http://localhost)
#   # address = "http://localhost"
#
#   ## List of urls to query.
#   # urls = ["http://localhost"]
  urls = [{% for url in telegraf_node_urls %}"{{ url }}"{% if not loop.last %},{% endif %}{% endfor %}]
#
#   ## Set http_proxy.
#   ## Telegraf uses the system wide proxy settings if it's is not set.
#   # http_proxy = "http://localhost:8888"
#
#   ## Set response_timeout (default 5 seconds)
#   # response_timeout = "5s"
#
#   ## HTTP Request Method
#   # method = "GET"
  method = "{{ telegraf_node_method }}"
#
#   ## Whether to follow redirects from the server (defaults to false)
#   # follow_redirects = false
#
#   ## Optional file with Bearer token
#   ## file content is added as an Authorization header
#   # bearer_token = "/path/to/file"
#
#   ## Optional HTTP Basic Auth Credentials
#   # username = "username"
#   # password = "pa$$word"
#
#   ## Optional HTTP Request Body
#   # body = '''
#   # {'fake':'data'}
#   # '''
  body = '''{{ telegraf_node_body }}'''
#
#   ## Optional HTTP Request Body Form
#   ## Key value pairs to encode and set at URL form. Can be used with the POST
#   ## method + application/x-www-form-urlencoded content type to replicate the
#   ## POSTFORM method.
#   # body_form = { "key": "value" }
#
#   ## Optional name of the field that will contain the body of the response.
#   ## By default it is set to an empty String indicating that the body's
#   ## content won't be added
#   # response_body_field = ''
#
#   ## Maximum allowed HTTP response body size in bytes.
#   ## 0 means to use the default of 32MiB.
#   ## If the response body size exceeds this limit a "body_read_error" will
#   ## be raised.
#   # response_body_max_size = "32MiB"
#
#   ## Optional substring or regex match in body of the response (case sensitive)
#   # response_string_match = "\"service_status\": \"up\""
#   # response_string_match = "ok"
#   # response_string_match = "\".*_status\".?:.?\"up\""
  response_string_match = "{{ telegraf_node_response_string_match }}"
#
#   ## Expected response status code.
#   ## The status code of the response is compared to this value. If they match,
#   ## the field "response_status_code_match" will be 1, otherwise it will be 0.
#   ## If the expected status code is 0, the check is disabled and the field
#   ## won't be added.
#   # response_status_code = 0
  response_status_code = {{ telegraf_node_response_code }}
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem"
#   # tls_cert = "/etc/telegraf/cert.pem"
#   # tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
#   ## Use the given name as the SNI server name on each URL
#   # tls_server_name = ""
#   ## TLS renegotiation method, choose from "never", "once", "freely"
#   # tls_renegotiation_method = "never"
#
#   ## HTTP Request Headers (all values must be strings)
#   # [inputs.http_response.headers]
#   #   Host = "github.com"
{% if telegraf_node_response_headers.items() | length > 0 %}
  [inputs.http_response.headers]
{% for key, value in telegraf_node_response_headers.items() | sort %}
    {{ key }} = "{{ value }}"
{% endfor %}
{% endif %}
#
#   ## Optional setting to map response http headers into tags
#   ## If the http header is not present on the request, no corresponding tag will
#   ## be added. If multiple instances of the http header are present, only the
#   ## first value will be used.
#   # http_header_tags = {"HTTP_HEADER" = "TAG_NAME"}
#
#   ## Interface to use when dialing an address
#   # interface = "eth0"
{% endif %}


{% if telegraf_health_check_urls | length > 0 and telegraf_node_urls | length == 0 %}
[[inputs.http_response]]
  urls = [{% for url in telegraf_health_check_urls %}"{{ url }}"{% if not loop.last %},{% endif %}{% endfor %}]
  response_status_code = 200
{% endif %}


{% if telegraf_mdstat_enable %}
# # Get kernel statistics from /proc/mdstat
# # This plugin ONLY supports Linux
[[inputs.mdstat]]
#   ## Sets file path
#   ## If not specified, then default is /proc/mdstat
#   # file_name = "/proc/mdstat"
{% endif %}


# # Read metrics about network interface usage
[[inputs.net]]
#   ## By default, telegraf gathers stats from any up interface (excluding loopback)
#   ## Setting interfaces will tell it to gather these explicit interfaces,
#   ## regardless of status.
#   ##
  interfaces = ["{{ ansible_default_ipv4.interface }}"]
#   ##
#   ## On linux systems telegraf also collects protocol stats.
#   ## Setting ignore_protocol_stats to true will skip reporting of protocol metrics.
#   ##
#   # ignore_protocol_stats = false
#   ##


# # Read TCP metrics such as established, time wait and sockets counts.
[[inputs.netstat]]
#   # no configuration


{% if telegraf_ping_hosts | length > 0 %}
# # Ping given url(s) and return statistics
[[inputs.ping]]
#   ## Hosts to send ping packets to.
#   urls = ["example.org"]
    urls = [{% for host in telegraf_ping_hosts %}"{{ host }}"{% if not loop.last %},{% endif %}{% endfor %}]
#
#   ## Method used for sending pings, can be either "exec" or "native".  When set
#   ## to "exec" the systems ping command will be executed.  When set to "native"
#   ## the plugin will send pings directly.
#   ##
#   ## While the default is "exec" for backwards compatibility, new deployments
#   ## are encouraged to use the "native" method for improved compatibility and
#   ## performance.
#   # method = "exec"
  method = "native"
#
#   ## Number of ping packets to send per interval.  Corresponds to the "-c"
#   ## option of the ping command.
#   # count = 1
  count = {{ telegraf_ping_count }}
#
#   ## Time to wait between sending ping packets in seconds.  Operates like the
#   ## "-i" option of the ping command.
#   # ping_interval = 1.0
  ping_interval = {{ telegraf_ping_interval }}
#
#   ## If set, the time to wait for a ping response in seconds.  Operates like
#   ## the "-W" option of the ping command.
#   # timeout = 1.0
  timeout = {{ telegraf_ping_timeout }}
#
#   ## If set, the total ping deadline, in seconds.  Operates like the -w option
#   ## of the ping command.
#   # deadline = 10
  deadline = {{ telegraf_ping_deadline }}
#
#   ## Interface or source address to send ping from.  Operates like the -I or -S
#   ## option of the ping command.
#   # interface = ""
#
#   ## Percentiles to calculate. This only works with the native method.
#   # percentiles = [50, 95, 99]
#
#   ## Specify the ping executable binary.
#   # binary = "ping"
#
#   ## Arguments for ping command. When arguments is not empty, the command from
#   ## the binary option will be used and other options (ping_interval, timeout,
#   ## etc) will be ignored.
#   # arguments = ["-c", "3"]
#
#   ## Use only IPv6 addresses when resolving a hostname.
#   # ipv6 = false
{% endif %}


{% if telegraf_disk_smart_enable %}
# # Read metrics from storage devices supporting S.M.A.R.T.
[[inputs.smart]]
#     ## Optionally specify the path to the smartctl executable
#     # path_smartctl = "/usr/bin/smartctl"
        path_smartctl = "/usr/sbin/smartctl"
#
#     ## Optionally specify the path to the nvme-cli executable
#     # path_nvme = "/usr/bin/nvme"
        path_nvme = "/usr/sbin/nvme"
#
#     ## Optionally specify if vendor specific attributes should be propagated for NVMe disk case
#     ## ["auto-on"] - automatically find and enable additional vendor specific disk info
#     ## ["vendor1", "vendor2", ...] - e.g. "Intel" enable additional Intel specific disk info
#     # enable_extensions = ["auto-on"]
#
#     ## On most platforms used cli utilities requires root access.
#     ## Setting 'use_sudo' to true will make use of sudo to run smartctl or nvme-cli.
#     ## Sudo must be configured to allow the telegraf user to run smartctl or nvme-cli
#     ## without a password.
#     # use_sudo = false
        use_sudo = true
#
#     ## Skip checking disks in this power mode. Defaults to
#     ## "standby" to not wake up disks that have stopped rotating.
#     ## See --nocheck in the man pages for smartctl.
#     ## smartctl version 5.41 and 5.42 have faulty detection of
#     ## power mode and might require changing this value to
#     ## "never" depending on your disks.
#     # nocheck = "standby"
#
#     ## Gather all returned S.M.A.R.T. attribute metrics and the detailed
#     ## information from each drive into the 'smart_attribute' measurement.
#     # attributes = false
        attributes = true
#
#     ## Optionally specify devices to exclude from reporting if disks auto-discovery is performed.
#     # excludes = [ "/dev/pass6" ]
#
#     ## Optionally specify devices and device type, if unset
#     ## a scan (smartctl --scan and smartctl --scan -d nvme) for S.M.A.R.T. devices will be done
#     ## and all found will be included except for the excluded in excludes.
#     # devices = [ "/dev/ada0 -d atacam", "/dev/nvme0"]
#
#     ## Timeout for the cli command to complete.
#     # timeout = "30s"
#
#     ## Optionally call smartctl and nvme-cli with a specific concurrency policy.
#     ## By default, smartctl and nvme-cli are called in separate threads (goroutines) to gather disk attributes.
#     ## Some devices (e.g. disks in RAID arrays) may have access limitations that require sequential reading of
#     ## SMART data - one individual array drive at the time. In such case please set this configuration option
#     ## to "sequential" to get readings for all drives.
#     ## valid options: concurrent, sequential
#     # read_method = "concurrent"
{% endif %}


# # Read metrics about temperature
[[inputs.temp]]
#   ## Desired output format (Linux only)
#   ## Available values are
#   ##   v1 -- use pre-v1.22.4 sensor naming, e.g. coretemp_core0_input
#   ##   v2 -- use v1.22.4+ sensor naming, e.g. coretemp_core_0_input
#   # metric_format = "v2"
#
#   ## Add device tag to distinguish devices with the same name (Linux only)
#   # add_device_tag = false



###############################################################################
#                            SERVICE INPUT PLUGINS                            #
###############################################################################


{% if telegraf_postgresql_enable %}
# # Read metrics from one or many postgresql servers
[[inputs.postgresql]]
#   ## Specify address via a url matching:
#   ##   postgres://[pqgotest[:password]]@localhost[/dbname]?sslmode=[disable|verify-ca|verify-full]&statement_timeout=...
#   ## or a simple string:
#   ##   host=localhost user=pqgotest password=... sslmode=... dbname=app_production
#   ## Users can pass the path to the socket as the host value to use a socket
#   ## connection (e.g. `/var/run/postgresql`).
#   ##
#   ## All connection parameters are optional.
#   ##
#   ## Without the dbname parameter, the driver will default to a database
#   ## with the same name as the user. This dbname is just for instantiating a
#   ## connection with the server and doesn't restrict the databases we are trying
#   ## to grab metrics for.
#   ##
#   address = "host=localhost user=postgres sslmode=disable"
  address = "host=localhost user={{ telegraf_postgresql_user }} password={{ telegraf_postgresql_password }} sslmode=disable"
#   ## A custom name for the database that will be used as the "server" tag in the
#   ## measurement output. If not specified, a default one generated from
#   ## the connection address is used.
#   # outputaddress = "db01"
#
#   ## connection configuration.
#   ## maxlifetime - specify the maximum lifetime of a connection.
#   ## default is forever (0s)
#   ##
#   ## Note that this does not interrupt queries, the lifetime will not be enforced
#   ## whilst a query is running
#   # max_lifetime = "0s"
#
#   ## A  list of databases to explicitly ignore.  If not specified, metrics for all
#   ## databases are gathered.  Do NOT use with the 'databases' option.
#   # ignored_databases = ["postgres", "template0", "template1"]
  ignored_databases = ["postgres", "template0", "template1"]
#
#   ## A list of databases to pull metrics about. If not specified, metrics for all
#   ## databases are gathered.  Do NOT use with the 'ignored_databases' option.
#   # databases = ["app_production", "testing"]
#
#   ## Whether to use prepared statements when connecting to the database.
#   ## This should be set to false when connecting through a PgBouncer instance
#   ## with pool_mode set to transaction.
#   prepared_statements = true
{% endif %}


{% if telegraf_prometheus_urls | length > 0 %}
# # Read metrics from one or many prometheus clients
 [[inputs.prometheus]]
#   ## An array of urls to scrape metrics from.
#   urls = ["http://localhost:9100/metrics"]
    urls = [{% for url in telegraf_prometheus_urls %}"{{ url }}"{% if not loop.last %},{% endif %}{% endfor %}]
#
#   ## Metric version controls the mapping from Prometheus metrics into Telegraf metrics.
#   ## See "Metric Format Configuration" in plugins/inputs/prometheus/README.md for details.
#   ## Valid options: 1, 2
#   # metric_version = 1
    metric_version = 2
#
#   ## Url tag name (tag containing scrapped url. optional, default is "url")
#   # url_tag = "url"
#
#   ## Whether the timestamp of the scraped metrics will be ignored.
#   ## If set to true, the gather time will be used.
#   # ignore_timestamp = false
#
#   ## An array of Kubernetes services to scrape metrics from.
#   # kubernetes_services = ["http://my-service-dns.my-namespace:9100/metrics"]
#
#   ## Kubernetes config file to create client from.
#   # kube_config = "/path/to/kubernetes.config"
#
#   ## Scrape Pods
#   ## Enable scraping of k8s pods. Further settings as to which pods to scape
#   ## are determiend by the 'method' option below. When enabled, the default is
#   ## to use annotations to determine whether to scrape or not.
#   # monitor_kubernetes_pods = false
#
#   ## Scrape Pods Method
#   ## annotations: default, looks for specific pod annotations documented below
#   ## settings: only look for pods matching the settings provided, not
#   ##   annotations
#   ## settings+annotations: looks at pods that match annotations using the user
#   ##   defined settings
#   # monitor_kubernetes_pods_method = "annotations"
#
#   ## Scrape Pods 'annotations' method options
#   ## If set method is set to 'annotations' or 'settings+annotations', these
#   ## annotation flags are looked for:
#   ## - prometheus.io/scrape: Required to enable scraping for this pod. Can also
#   ##     use 'prometheus.io/scrape=false' annotation to opt-out entirely.
#   ## - prometheus.io/scheme: If the metrics endpoint is secured then you will
#   ##     need to set this to 'https' & most likely set the tls config
#   ## - prometheus.io/path: If the metrics path is not /metrics, define it with
#   ##     this annotation
#   ## - prometheus.io/port: If port is not 9102 use this annotation
#
#   ## Scrape Pods 'settings' method options
#   ## When using 'settings' or 'settings+annotations', the default values for
#   ## annotations can be modified using with the following options:
#   # monitor_kubernetes_pods_scheme = "http"
#   # monitor_kubernetes_pods_port = "9102"
#   # monitor_kubernetes_pods_path = "/metrics"
#
#   ## Get the list of pods to scrape with either the scope of
#   ## - cluster: the kubernetes watch api (default, no need to specify)
#   ## - node: the local cadvisor api; for scalability. Note that the config node_ip or the environment variable NODE_IP must be set to the host IP.
#   # pod_scrape_scope = "cluster"
#
#   ## Only for node scrape scope: node IP of the node that telegraf is running on.
#   ## Either this config or the environment variable NODE_IP must be set.
#   # node_ip = "10.180.1.1"
#
#   ## Only for node scrape scope: interval in seconds for how often to get updated pod list for scraping.
#   ## Default is 60 seconds.
#   # pod_scrape_interval = 60
#
#   ## Restricts Kubernetes monitoring to a single namespace
#   ##   ex: monitor_kubernetes_pods_namespace = "default"
#   # monitor_kubernetes_pods_namespace = ""
#   ## The name of the label for the pod that is being scraped.
#   ## Default is 'namespace' but this can conflict with metrics that have the label 'namespace'
#   # pod_namespace_label_name = "namespace"
#   # label selector to target pods which have the label
#   # kubernetes_label_selector = "env=dev,app=nginx"
#   # field selector to target pods
#   # eg. To scrape pods on a specific node
#   # kubernetes_field_selector = "spec.nodeName=$HOSTNAME"
#
#   ## Filter which pod annotations and labels will be added to metric tags
#   #
#   # pod_annotation_include = ["annotation-key-1"]
#   # pod_annotation_exclude = ["exclude-me"]
#   # pod_label_include = ["label-key-1"]
#   # pod_label_exclude = ["exclude-me"]
#
#   # cache refresh interval to set the interval for re-sync of pods list.
#   # Default is 60 minutes.
#   # cache_refresh_interval = 60
#
#   ## Scrape Services available in Consul Catalog
#   # [inputs.prometheus.consul]
#   #   enabled = true
#   #   agent = "http://localhost:8500"
#   #   query_interval = "5m"
#
#   #   [[inputs.prometheus.consul.query]]
#   #     name = "a service name"
#   #     tag = "a service tag"
#   #     url = 'http://{ {if ne .ServiceAddress ""} }{ {.ServiceAddress} }{ {else} }{ {.Address} }{ {end} }:{ {.ServicePort} }/{ {with .ServiceMeta.metrics_path} }{ {.} }{ {else} }metrics{ {end} }'
#   #     [inputs.prometheus.consul.query.tags]
#   #       host = "{ {.Node} }"
#
#   ## Use bearer token for authorization. ('bearer_token' takes priority)
#   # bearer_token = "/path/to/bearer/token"
#   ## OR
#   # bearer_token_string = "abc_123"
#
#   ## HTTP Basic Authentication username and password. ('bearer_token' and
#   ## 'bearer_token_string' take priority)
#   # username = ""
#   # password = ""
#
#   ## Optional custom HTTP headers
#   # http_headers = {"X-Special-Header" = "Special-Value"}
#
#   ## Specify timeout duration for slower prometheus clients (default is 5s)
#   # timeout = "5s"
#
#   ## deprecated in 1.26; use the timeout option
#   # response_timeout = "5s"
#
#   ## HTTP Proxy support
#   # use_system_proxy = false
#   # http_proxy_url = ""
#
#   ## Optional TLS Config
#   # tls_ca = /path/to/cafile
#   # tls_cert = /path/to/certfile
#   # tls_key = /path/to/keyfile
#
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
#
#   ## Use the given name as the SNI server name on each URL
#   # tls_server_name = "myhost.example.org"
#
#   ## TLS renegotiation method, choose from "never", "once", "freely"
#   # tls_renegotiation_method = "never"
#
#   ## Enable/disable TLS
#   ## Set to true/false to enforce TLS being enabled/disabled. If not set,
#   ## enable TLS only if any of the other options are specified.
#   # tls_enable = true
#
#   ## Control pod scraping based on pod namespace annotations
#   ## Pass and drop here act like tagpass and tagdrop, but instead
#   ## of filtering metrics they filters pod candidates for scraping
#   #[inputs.prometheus.namespace_annotation_pass]
#   # annotation_key = ["value1", "value2"]
#   #[inputs.prometheus.namespace_annotation_drop]
#   # some_annotation_key = ["dont-scrape"]
{% endif %}


{% if telegraf_rpi_temp_enable %}
[[inputs.file]]
  files = ["/sys/class/thermal/thermal_zone0/temp"]
  name_override = "rpi_cpu_temperature"
  data_format = "value"
  data_type = "integer"

[[inputs.exec]]
  commands = [ "/usr/bin/vcgencmd measure_temp" ]
  name_override = "rpi_gpu_temperature"
  data_format = "grok"
  grok_patterns = ["%{NUMBER:value:float}"]
{% endif %}


[[inputs.exec]]
  commands = [
    "/bin/sh -c 'echo \"linux_kernel_info,version=$(cat /proc/sys/kernel/osrelease | tr -d \"\\n\")\" value=1'"
  ]
  data_format = "influx"
